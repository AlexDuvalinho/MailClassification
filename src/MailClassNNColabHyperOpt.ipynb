{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3l1iaLx08WYR"
   },
   "source": [
    "#Perform space hyperparameters grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iffbMjuZ3iCM"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vHKbn1Zl7Omi"
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1557,
     "status": "ok",
     "timestamp": 1573431221296,
     "user": {
      "displayName": "Sébastien Saubert",
      "photoUrl": "",
      "userId": "10340327150525262013"
     },
     "user_tz": -60
    },
    "id": "IsY_FIQFWisV",
    "outputId": "0154ab5e-5294-4c2b-e05e-ad4a2042cf87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1gGqWAs3yoej"
   },
   "outputs": [],
   "source": [
    "# Define searched space\n",
    "space = {\n",
    "    'batchsize': hp.choice('batchsize', [16, 32, 64, 128, 256, 512, 1024]),\n",
    "    'epoch': hp.choice('epoch', [10, 20, 30]),\n",
    "    'nfold': 5,\n",
    "    'PCA_comp' : hp.choice('PCA_comp', [None, 90, 100, 110, 80, 70]),\n",
    "    'lr' : hp.choice('lr', [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1]),\n",
    "    'beta1' : hp.choice('beta1', [0.9, 0.7, 0.5, 0.3, 0.1]),\n",
    "    'beta2' : hp.choice('beta2', [0.999,0.995,0.99, 0.95]),\n",
    "    'amsgrad' : hp.choice('amsgrad', [True, False]),\n",
    "    'activation' : hp.choice('activation', ['leakyRelu', 'elu', 'linear', 'sigmoid', 'relu'])      \n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUO1hIxov9wv"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cCxcgqk3XJYT"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_eng.csv')\n",
    "test = pd.read_csv('test_eng.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1854,
     "status": "ok",
     "timestamp": 1573431221616,
     "user": {
      "displayName": "Sébastien Saubert",
      "photoUrl": "",
      "userId": "10340327150525262013"
     },
     "user_tz": -60
    },
    "id": "P77uoJ6fEpmQ",
    "outputId": "4ea7f7be-8da7-46e7-c281-b62e2bfa39bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train.isnull().sum().sum())\n",
    "print(test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jh5YCFUcwY6W"
   },
   "source": [
    "## Apply Kfold Split\n",
    "Instead of looping on kf.split, take the nfold test split and merge 4 of them to build train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1843,
     "status": "ok",
     "timestamp": 1573431221617,
     "user": {
      "displayName": "Sébastien Saubert",
      "photoUrl": "",
      "userId": "10340327150525262013"
     },
     "user_tz": -60
    },
    "id": "QSghdjVLwR49",
    "outputId": "d71653ef-a8e6-4098-f068-e88070d6532e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4968, 141)\n",
      "(4968, 141)\n",
      "(4968, 141)\n",
      "(4968, 141)\n",
      "(4968, 141)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=space['nfold'], random_state=750, shuffle=True)\n",
    "train_folds = [None for x in range(space['nfold'])]\n",
    "\n",
    "i=0\n",
    "for _, test_indexed in kf.split(train):\n",
    "  train_folds[i] = train.iloc[test_indexed,:]\n",
    "  i += 1\n",
    "\n",
    "for item in train_folds:\n",
    "  print(item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ITrh1myBblv"
   },
   "outputs": [],
   "source": [
    "def PowerData(df):\n",
    "    '''\n",
    "    add for all none categorical data it's square\n",
    "    '''\n",
    "    return df\n",
    "    \n",
    "    square_columns = [col for col in df.columns[~df.columns.str.startswith('Cat_')]]\n",
    "    \n",
    "    #print('Nb columns to square :', len(square_columns))\n",
    "    \n",
    "    for col in square_columns:\n",
    "        df[f'{col}_x2'] = df[col]**2\n",
    "        df[f'{col}_x3'] = df[col]**3\n",
    "        df[f'{col}_x0_5'] = df[col]**0.5\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3owUy_tr6LGA"
   },
   "source": [
    "Build a normalize that will apply standard scaler and PCA if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wXqw1knt-kDR"
   },
   "outputs": [],
   "source": [
    "def NormalizeData(train, CVorTest, PCA_comp = None, ScaleCat = False):\n",
    "    '''\n",
    "    Normalize data using a standard scaler\n",
    "    train:\n",
    "        dataframe that will be use to fit and transformed by the scaler and PCA\n",
    "    CVorTest:\n",
    "        dataframe that will be transformed the scaler and PCA\n",
    "    PCA_comp:\n",
    "        Number of PCA components to keep, if None, PCA not applied\n",
    "    ScaleCat:\n",
    "        Scale or not the categorical columns with the standard scaler\n",
    "    '''\n",
    "    \n",
    "    sc = StandardScaler()\n",
    "    \n",
    "    if ScaleCat:\n",
    "        scale_columns = train.columns\n",
    "    else:\n",
    "        scale_columns = [col for col in train.columns[~train.columns.str.startswith('Cat_')]]\n",
    "          \n",
    "    #perform feature scaling    \n",
    "    #print('Nb columns to scale :', len(scale_columns))\n",
    "    train.loc[:, scale_columns] = sc.fit_transform(train.loc[:, scale_columns]) \n",
    "    CVorTest.loc[:, scale_columns] = sc.transform(CVorTest.loc[:, scale_columns]) \n",
    "    \n",
    "    if PCA_comp is None:\n",
    "        return train.values, CVorTest.values\n",
    "    \n",
    "    pca = PCA(PCA_comp)\n",
    "    train = pca.fit_transform(train)\n",
    "    CVorTest = pca.transform(CVorTest)\n",
    "    \n",
    "    return train, CVorTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "siSS80qeXTzb"
   },
   "outputs": [],
   "source": [
    "def BuildModel(nb_features, activation = 'leakyRelu', Batchnorm=True):\n",
    "  '''\n",
    "    Build NN model\n",
    "    nb_features:\n",
    "      number of features in input\n",
    "    Batchnorm:\n",
    "      Apply or not batchnorm in hidden layers\n",
    "    activation:\n",
    "      Activation to use in hidden layer : leakyRelu, relu, sigmoid, linear, elu\n",
    "  '''\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  layers_dim = [512, 1024, 256, 128, 128, 128, 64]\n",
    "\n",
    "  for dim in layers_dim:\n",
    "    model.add(tf.keras.layers.Dense(dim, input_shape=(nb_features,)))\n",
    "    if Batchnorm:\n",
    "      model.add(tf.keras.layers.BatchNormalization())\n",
    "    if activation == 'leakyRelu':\n",
    "      model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    else:\n",
    "      model.add(tf.keras.layers.Activation(activation))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(4))\n",
    "  model.add(tf.keras.layers.Activation('softmax'))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mdQep6qXf-kq"
   },
   "outputs": [],
   "source": [
    "#metrics to compare model with Kaggle scoring\n",
    "def f1_macro(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(tf.round(y_pred), tf.float64)\n",
    "\n",
    "    TP = tf.cast(tf.math.count_nonzero(y_pred * y_true, axis=None), tf.float64)\n",
    "    FP = tf.cast(tf.math.count_nonzero(y_pred * (y_true - 1), axis=None), tf.float64)\n",
    "    FN = tf.cast(tf.math.count_nonzero((y_pred - 1) * y_true, axis=None), tf.float64)\n",
    "    \n",
    "    precision = tf.math.divide_no_nan(TP, TP + FP)\n",
    "    recall = tf.math.divide_no_nan(TP, TP + FN)\n",
    "    f1 = tf.math.divide_no_nan(2 * precision * recall,precision + recall)\n",
    "    return tf.reduce_mean(f1)\n",
    "\n",
    "def f1_micro(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(tf.round(y_pred), tf.float64)\n",
    "\n",
    "    TP = tf.cast(tf.math.count_nonzero(y_pred * y_true, axis=0), tf.float64)\n",
    "    FP = tf.cast(tf.math.count_nonzero(y_pred * (y_true - 1), axis=0), tf.float64)\n",
    "    FN = tf.cast(tf.math.count_nonzero((y_pred - 1) * y_true, axis=0), tf.float64)\n",
    "    \n",
    "    precision = tf.math.divide_no_nan(TP, TP + FP)\n",
    "    recall = tf.math.divide_no_nan(TP, TP + FN)\n",
    "    f1 = tf.math.divide_no_nan(2 * precision * recall,precision + recall)\n",
    "    return tf.reduce_mean(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p2Twzs4qYdt"
   },
   "source": [
    "## Performing the gridsearch\n",
    "Will use a train/CV split : 80/20\n",
    "\n",
    "Results will be store on a csv file on google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1804,
     "status": "ok",
     "timestamp": 1573431221619,
     "user": {
      "displayName": "Sébastien Saubert",
      "photoUrl": "",
      "userId": "10340327150525262013"
     },
     "user_tz": -60
    },
    "id": "M75kO6oJS8xK",
    "outputId": "160dd36d-6f97-4f5e-f7f8-6c416cfc2f62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "save_file = '/content/drive/My Drive/Kaggle/EmailClass/gridSearch.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NA5kRstlqXku"
   },
   "outputs": [],
   "source": [
    "def evaluateModel(params):\n",
    "  nfolds = params['nfold']\n",
    "  loss_CV_folds = [None for i in range(nfolds)]\n",
    "  f1_macro_CV_folds = [None for i in range(nfolds)]\n",
    "  f1_micro_CV_folds = [None for i in range(nfolds)]\n",
    "\n",
    "  for i in range(nfolds):\n",
    "    CV_block = i\n",
    "    train_blocks = list(range(nfolds))\n",
    "    train_blocks.pop(i)\n",
    "\n",
    "    train = pd.concat([train_folds[j].copy() for j in train_blocks])\n",
    "    CV = train_folds[CV_block].copy()\n",
    "\n",
    "    #extract labels & convert to categorical\n",
    "    labels_train = pd.get_dummies(train['label'])\n",
    "    labels_CV = pd.get_dummies(CV['label'])\n",
    "\n",
    "    #remove labels from train\n",
    "    train.drop(columns=['label'], inplace=True)\n",
    "    CV.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    #apply normalization\n",
    "    train, CV = NormalizeData(PowerData(train), PowerData(CV),PCA_comp=params['PCA_comp'])\n",
    "\n",
    "    #build model\n",
    "    model = BuildModel(train.shape[1], activation=params['activation'])\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=params['lr'], beta_1=params['beta1'], beta_2=params['beta2'], amsgrad=params['amsgrad'])\n",
    "    model.compile('adam',loss='categorical_crossentropy', metrics=[f1_macro, f1_micro])\n",
    "\n",
    "    #run model\n",
    "    hist = model.fit(train, labels_train.values, validation_data=(CV, labels_CV.values), \n",
    "            epochs = params['epoch'], verbose = 0, batch_size=params['batchsize'])\n",
    "    \n",
    "    result = model.evaluate(CV, labels_CV.values, batch_size=params['batchsize'], verbose=0)\n",
    "    loss_CV_folds[i] = result[0]\n",
    "    f1_macro_CV_folds[i] = result[1]\n",
    "    f1_micro_CV_folds[i] = result[2]\n",
    "\n",
    "  print(params)\n",
    "  print(f1_micro_CV_folds)\n",
    "\n",
    "  params['CV loss'] = np.asarray(loss_CV_folds).mean()\n",
    "  params['CV FScore macro'] = np.asarray(f1_macro_CV_folds).mean()\n",
    "  params['CV FScore micro'] = np.asarray(f1_micro_CV_folds).mean()\n",
    "  \n",
    "  with open (save_file, 'a+') as fp:\n",
    "        fp.write(str(params) +'\\n')\n",
    "\n",
    "  return {\n",
    "        'loss': 1- params['CV FScore micro'],\n",
    "        'status': STATUS_OK,\n",
    "        'stats_running': STATUS_RUNNING\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "jkNP48Bc__Cl",
    "outputId": "5db2786b-1ecc-469f-875b-47ffd16cc23a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PCA_comp': None, 'activation': 'sigmoid', 'amsgrad': False, 'batchsize': 256, 'beta1': 0.1, 'beta2': 0.99, 'epoch': 10, 'lr': 0.003, 'nfold': 5}\n",
      "[0.932103, 0.93260354, 0.9388524, 0.9274831, 0.9301233]\n",
      "{'PCA_comp': None, 'activation': 'sigmoid', 'amsgrad': False, 'batchsize': 256, 'beta1': 0.1, 'beta2': 0.99, 'epoch': 10, 'lr': 0.003, 'nfold': 5}\n",
      "[0.932776, 0.9369172, 0.9352912, 0.9285185, 0.9242047]\n",
      "{'PCA_comp': 100, 'activation': 'sigmoid', 'amsgrad': False, 'batchsize': 16, 'beta1': 0.9, 'beta2': 0.95, 'epoch': 20, 'lr': 0.003, 'nfold': 5}\n",
      "[0.8984261, 0.9082019, 0.9021843, 0.89635915, 0.91100997]\n",
      "{'PCA_comp': None, 'activation': 'elu', 'amsgrad': True, 'batchsize': 512, 'beta1': 0.5, 'beta2': 0.99, 'epoch': 30, 'lr': 0.0001, 'nfold': 5}\n",
      "[0.94457567, 0.94937074, 0.94813186, 0.94039774, 0.9392177]\n",
      "{'PCA_comp': 90, 'activation': 'elu', 'amsgrad': True, 'batchsize': 512, 'beta1': 0.5, 'beta2': 0.995, 'epoch': 30, 'lr': 0.0001, 'nfold': 5}\n",
      "[0.9425166, 0.94393605, 0.94421625, 0.93742764, 0.9411681]\n",
      "{'PCA_comp': 80, 'activation': 'leakyRelu', 'amsgrad': True, 'batchsize': 32, 'beta1': 0.7, 'beta2': 0.999, 'epoch': 30, 'lr': 0.001, 'nfold': 5}\n",
      "[0.9395659, 0.9428982, 0.9473205, 0.94201434, 0.9377037]\n",
      "{'PCA_comp': 110, 'activation': 'elu', 'amsgrad': True, 'batchsize': 128, 'beta1': 0.3, 'beta2': 0.99, 'epoch': 30, 'lr': 0.0003, 'nfold': 5}\n",
      "[0.9440418, 0.93770325, 0.94795835, 0.9406143, 0.93671006]\n",
      "  7%|▋         | 7/100 [53:48<12:33:35, 486.19s/it, best loss: 0.05566132068634033]"
     ]
    }
   ],
   "source": [
    "# Trail\n",
    "trials = Trials()\n",
    "\n",
    "# Set algoritm parameters\n",
    "algo = partial(tpe.suggest, \n",
    "               n_startup_jobs=-1)\n",
    "\n",
    "# Seting the number of Evals\n",
    "MAX_EVALS= 100\n",
    "\n",
    "# Fit Tree Parzen Estimator\n",
    "best_vals = fmin(evaluateModel, space=space, verbose=1,\n",
    "                 algo=algo, max_evals=MAX_EVALS, trials=trials)\n",
    "\n",
    "# Print best parameters\n",
    "best_params = space_eval(space, best_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "57axRCciD6yY"
   },
   "outputs": [],
   "source": [
    "best_params"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MailClassNNColabHyperOpt.ipynb",
   "provenance": [
    {
     "file_id": "1BbG5ho7RhbPhckkDoPBci0LUSJrH4m30",
     "timestamp": 1573382820519
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
