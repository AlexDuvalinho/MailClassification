{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iffbMjuZ3iCM",
    "outputId": "1a3de00d-76cb-4a98-e475-095ff4ac82e2"
   },
   "source": [
    "%tensorflow_version 2.x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IsY_FIQFWisV",
    "outputId": "03ece649-e89c-40b8-f199-309d8d81c3c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1gGqWAs3yoej"
   },
   "outputs": [],
   "source": [
    "# Define parameters (identify by gridsearch)\n",
    "params = {\n",
    "    'PCA_comp': None,\n",
    " 'activation': 'elu',\n",
    " 'amsgrad': True,\n",
    " 'batchsize': 512,\n",
    " 'beta1': 0.5,\n",
    " 'beta2': 0.99,\n",
    " 'epoch': 30,\n",
    " 'lr': 0.0001,\n",
    " 'nfold': 5  \n",
    " }\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUO1hIxov9wv"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cCxcgqk3XJYT"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/generated/train_eng.csv')\n",
    "test = pd.read_csv('../data/generated/test_eng.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "g8bG6oQXVFlH",
    "outputId": "8d9689de-4cd0-4e82-9c53-54e85a9e5601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#control there's no Null values\n",
    "print(train.isnull().sum().sum())\n",
    "print(test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jh5YCFUcwY6W"
   },
   "source": [
    "##Apply Kfold Split\n",
    "Instead of looping on kf.split, take the nfold test split and merge 4 of them to build train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "QSghdjVLwR49",
    "outputId": "cd58a00b-23de-49d8-8d4e-8d174a97e151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4968, 141)\n",
      "(4968, 141)\n",
      "(4968, 141)\n",
      "(4968, 141)\n",
      "(4968, 141)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=params['nfold'], random_state=750, shuffle=True)\n",
    "train_folds = [None for x in range(params['nfold'])]\n",
    "\n",
    "i=0\n",
    "for _, test_indexed in kf.split(train):\n",
    "  train_folds[i] = train.iloc[test_indexed,:]\n",
    "  i += 1\n",
    "\n",
    "for item in train_folds:\n",
    "  print(item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ITrh1myBblv"
   },
   "outputs": [],
   "source": [
    "def PowerData(df):\n",
    "    '''\n",
    "    add for all none categorical data it's square\n",
    "    '''\n",
    "    #hack\n",
    "    return df\n",
    "\n",
    "    square_columns = [col for col in df.columns[~df.columns.str.startswith('Cat_')]]\n",
    "    \n",
    "    print('Nb columns to square :', len(square_columns))\n",
    "    \n",
    "    for col in square_columns:\n",
    "        df[f'{col}_x2'] = df[col]**2\n",
    "        df[f'{col}_x3'] = df[col]**3\n",
    "        df[f'{col}_x0_5'] = df[col]**0.5\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3owUy_tr6LGA"
   },
   "source": [
    "Build a normalize that will apply standard scaler and PCA if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wXqw1knt-kDR"
   },
   "outputs": [],
   "source": [
    "def NormalizeData(train, CVorTest, PCA_comp = None, ScaleCat = False):\n",
    "    '''\n",
    "    Normalize data using a standard scaler\n",
    "    train:\n",
    "        dataframe that will be use to fit and transformed by the scaler and PCA\n",
    "    CVorTest:\n",
    "        dataframe that will be transformed the scaler and PCA\n",
    "    PCA_comp:\n",
    "        Number of PCA components to keep, if None, PCA not applied\n",
    "    ScaleCat:\n",
    "        Scale or not the categorical columns with the standard scaler\n",
    "    '''\n",
    "    \n",
    "    sc = StandardScaler()\n",
    "    \n",
    "    if ScaleCat:\n",
    "        scale_columns = train.columns\n",
    "    else:\n",
    "        scale_columns = [col for col in train.columns[~train.columns.str.startswith('Cat_')]]\n",
    "          \n",
    "    #perform feature scaling    \n",
    "    print('Nb columns to scale :', len(scale_columns))\n",
    "    train.loc[:, scale_columns] = sc.fit_transform(train.loc[:, scale_columns]) \n",
    "    CVorTest.loc[:, scale_columns] = sc.transform(CVorTest.loc[:, scale_columns]) \n",
    "    \n",
    "    if PCA_comp is None:\n",
    "        return train.values, CVorTest.values\n",
    "    \n",
    "    pca = PCA(PCA_comp)\n",
    "    train = pca.fit_transform(train)\n",
    "    CVorTest = pca.transform(CVorTest)\n",
    "    \n",
    "    return train, CVorTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "siSS80qeXTzb"
   },
   "outputs": [],
   "source": [
    "def BuildModel(nb_features, activation = 'leakyRelu', Batchnorm=True):\n",
    "  '''\n",
    "    Build NN model\n",
    "    nb_features:\n",
    "      number of features in input\n",
    "    Batchnorm:\n",
    "      Apply or not batchnorm in hidden layers\n",
    "    activation:\n",
    "      Activation to use in hidden layer : leakyRelu, relu, sigmoid, linear, elu\n",
    "  '''\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  layers_dim = [512, 1024, 256, 128, 128, 128, 64]\n",
    "\n",
    "  for dim in layers_dim:\n",
    "    model.add(tf.keras.layers.Dense(dim, input_shape=(nb_features,)))\n",
    "    if Batchnorm:\n",
    "      model.add(tf.keras.layers.BatchNormalization())\n",
    "    if activation == 'leakyRelu':\n",
    "      model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
    "    else:\n",
    "      model.add(tf.keras.layers.Activation(activation))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(4))\n",
    "  model.add(tf.keras.layers.Activation('softmax'))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mdQep6qXf-kq"
   },
   "outputs": [],
   "source": [
    "#metrics to compare model with Kaggle scoring\n",
    "def f1_macro(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(tf.round(y_pred), tf.float64)\n",
    "\n",
    "    TP = tf.cast(tf.math.count_nonzero(y_pred * y_true, axis=None), tf.float64)\n",
    "    FP = tf.cast(tf.math.count_nonzero(y_pred * (y_true - 1), axis=None), tf.float64)\n",
    "    FN = tf.cast(tf.math.count_nonzero((y_pred - 1) * y_true, axis=None), tf.float64)\n",
    "    \n",
    "    precision = tf.math.divide_no_nan(TP, TP + FP)\n",
    "    recall = tf.math.divide_no_nan(TP, TP + FN)\n",
    "    f1 = tf.math.divide_no_nan(2 * precision * recall,precision + recall)\n",
    "    return tf.reduce_mean(f1)\n",
    "\n",
    "def f1_micro(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(tf.round(y_pred), tf.float64)\n",
    "\n",
    "    TP = tf.cast(tf.math.count_nonzero(y_pred * y_true, axis=0), tf.float64)\n",
    "    FP = tf.cast(tf.math.count_nonzero(y_pred * (y_true - 1), axis=0), tf.float64)\n",
    "    FN = tf.cast(tf.math.count_nonzero((y_pred - 1) * y_true, axis=0), tf.float64)\n",
    "    \n",
    "    precision = tf.math.divide_no_nan(TP, TP + FP)\n",
    "    recall = tf.math.divide_no_nan(TP, TP + FN)\n",
    "    f1 = tf.math.divide_no_nan(2 * precision * recall,precision + recall)\n",
    "    return tf.reduce_mean(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p2Twzs4qYdt"
   },
   "source": [
    "## Performing the KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NA5kRstlqXku",
    "outputId": "fccb59ee-a4b7-4755-904b-e779a8945ecf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing KFold iteration on chunk 1/5\n",
      "Nb columns to scale : 58\n",
      "train shape (19872, 140) train label shape (19872, 4)\n",
      "CV shape (4968, 140) CV shape (4968, 4)\n",
      "Epochs : 30 Batch size : 512 Step per epochs : 38\n",
      "Train on 19872 samples, validate on 4968 samples\n",
      "Epoch 1/30\n",
      "19872/19872 [==============================] - 10s 497us/sample - loss: 0.3650 - f1_macro: 0.8635 - f1_micro: 0.8754 - val_loss: 0.3186 - val_f1_macro: 0.8801 - val_f1_micro: 0.8922\n",
      "Epoch 2/30\n",
      "19872/19872 [==============================] - 5s 265us/sample - loss: 0.2150 - f1_macro: 0.9165 - f1_micro: 0.9272 - val_loss: 0.2669 - val_f1_macro: 0.8895 - val_f1_micro: 0.9000\n",
      "Epoch 3/30\n",
      "19872/19872 [==============================] - 6s 282us/sample - loss: 0.1869 - f1_macro: 0.9272 - f1_micro: 0.9373 - val_loss: 0.2472 - val_f1_macro: 0.9027 - val_f1_micro: 0.9097\n",
      "Epoch 4/30\n",
      "19872/19872 [==============================] - 6s 284us/sample - loss: 0.1685 - f1_macro: 0.9357 - f1_micro: 0.9446 - val_loss: 0.2766 - val_f1_macro: 0.8926 - val_f1_micro: 0.9018\n",
      "Epoch 5/30\n",
      "19872/19872 [==============================] - 6s 317us/sample - loss: 0.1609 - f1_macro: 0.9375 - f1_micro: 0.9459 - val_loss: 0.2331 - val_f1_macro: 0.9117 - val_f1_micro: 0.9215\n",
      "Epoch 6/30\n",
      "19872/19872 [==============================] - 6s 311us/sample - loss: 0.1513 - f1_macro: 0.9423 - f1_micro: 0.9501 - val_loss: 0.1956 - val_f1_macro: 0.9210 - val_f1_micro: 0.9310\n",
      "Epoch 7/30\n",
      "19872/19872 [==============================] - 6s 306us/sample - loss: 0.1442 - f1_macro: 0.9437 - f1_micro: 0.9512 - val_loss: 0.1883 - val_f1_macro: 0.9281 - val_f1_micro: 0.9376\n",
      "Epoch 8/30\n",
      "19872/19872 [==============================] - 6s 302us/sample - loss: 0.1345 - f1_macro: 0.9492 - f1_micro: 0.9563 - val_loss: 0.1977 - val_f1_macro: 0.9234 - val_f1_micro: 0.9316\n",
      "Epoch 9/30\n",
      "19872/19872 [==============================] - 6s 292us/sample - loss: 0.1233 - f1_macro: 0.9543 - f1_micro: 0.9605 - val_loss: 0.1824 - val_f1_macro: 0.9339 - val_f1_micro: 0.9434\n",
      "Epoch 10/30\n",
      "19872/19872 [==============================] - 6s 291us/sample - loss: 0.1099 - f1_macro: 0.9591 - f1_micro: 0.9649 - val_loss: 0.1843 - val_f1_macro: 0.9337 - val_f1_micro: 0.9437\n",
      "Epoch 11/30\n",
      "19872/19872 [==============================] - 6s 295us/sample - loss: 0.1113 - f1_macro: 0.9576 - f1_micro: 0.9637 - val_loss: 0.1939 - val_f1_macro: 0.9310 - val_f1_micro: 0.9416\n",
      "Epoch 12/30\n",
      "19872/19872 [==============================] - 6s 296us/sample - loss: 0.1071 - f1_macro: 0.9587 - f1_micro: 0.9642 - val_loss: 0.2109 - val_f1_macro: 0.9250 - val_f1_micro: 0.9349\n",
      "Epoch 13/30\n",
      "19872/19872 [==============================] - 6s 288us/sample - loss: 0.1074 - f1_macro: 0.9589 - f1_micro: 0.9644 - val_loss: 0.2158 - val_f1_macro: 0.9222 - val_f1_micro: 0.9342\n",
      "Epoch 14/30\n",
      "19872/19872 [==============================] - 6s 298us/sample - loss: 0.0963 - f1_macro: 0.9630 - f1_micro: 0.9682 - val_loss: 0.2096 - val_f1_macro: 0.9281 - val_f1_micro: 0.9383\n",
      "Epoch 15/30\n",
      "19872/19872 [==============================] - 6s 283us/sample - loss: 0.0912 - f1_macro: 0.9674 - f1_micro: 0.9719 - val_loss: 0.2142 - val_f1_macro: 0.9229 - val_f1_micro: 0.9358\n",
      "Epoch 16/30\n",
      "19872/19872 [==============================] - 6s 280us/sample - loss: 0.0894 - f1_macro: 0.9664 - f1_micro: 0.9710 - val_loss: 0.2078 - val_f1_macro: 0.9324 - val_f1_micro: 0.9423\n",
      "Epoch 17/30\n",
      "19872/19872 [==============================] - 6s 297us/sample - loss: 0.0814 - f1_macro: 0.9696 - f1_micro: 0.9740 - val_loss: 0.2197 - val_f1_macro: 0.9299 - val_f1_micro: 0.9390\n",
      "Epoch 18/30\n",
      "19872/19872 [==============================] - 6s 302us/sample - loss: 0.0804 - f1_macro: 0.9704 - f1_micro: 0.9744 - val_loss: 0.2146 - val_f1_macro: 0.9353 - val_f1_micro: 0.9451\n",
      "Epoch 19/30\n",
      "19872/19872 [==============================] - 6s 290us/sample - loss: 0.0740 - f1_macro: 0.9724 - f1_micro: 0.9765 - val_loss: 0.2392 - val_f1_macro: 0.9275 - val_f1_micro: 0.9380\n",
      "Epoch 20/30\n",
      "19872/19872 [==============================] - 6s 281us/sample - loss: 0.0711 - f1_macro: 0.9723 - f1_micro: 0.9765 - val_loss: 0.2342 - val_f1_macro: 0.9306 - val_f1_micro: 0.9402\n",
      "Epoch 21/30\n",
      "19872/19872 [==============================] - 6s 279us/sample - loss: 0.0713 - f1_macro: 0.9726 - f1_micro: 0.9766 - val_loss: 0.2360 - val_f1_macro: 0.9242 - val_f1_micro: 0.9348\n",
      "Epoch 22/30\n",
      "19872/19872 [==============================] - 5s 276us/sample - loss: 0.0641 - f1_macro: 0.9768 - f1_micro: 0.9801 - val_loss: 0.2267 - val_f1_macro: 0.9338 - val_f1_micro: 0.9433\n",
      "Epoch 23/30\n",
      "19872/19872 [==============================] - 6s 281us/sample - loss: 0.0667 - f1_macro: 0.9743 - f1_micro: 0.9781 - val_loss: 0.2544 - val_f1_macro: 0.9238 - val_f1_micro: 0.9349\n",
      "Epoch 24/30\n",
      "19872/19872 [==============================] - 6s 308us/sample - loss: 0.0589 - f1_macro: 0.9777 - f1_micro: 0.9811 - val_loss: 0.2443 - val_f1_macro: 0.9320 - val_f1_micro: 0.9418\n",
      "Epoch 25/30\n",
      "19872/19872 [==============================] - 6s 290us/sample - loss: 0.0547 - f1_macro: 0.9791 - f1_micro: 0.9824 - val_loss: 0.2551 - val_f1_macro: 0.9276 - val_f1_micro: 0.9383\n",
      "Epoch 26/30\n",
      "19872/19872 [==============================] - 6s 286us/sample - loss: 0.0522 - f1_macro: 0.9811 - f1_micro: 0.9840 - val_loss: 0.2587 - val_f1_macro: 0.9313 - val_f1_micro: 0.9412\n",
      "Epoch 27/30\n",
      "19872/19872 [==============================] - 6s 307us/sample - loss: 0.0583 - f1_macro: 0.9785 - f1_micro: 0.9816 - val_loss: 0.2638 - val_f1_macro: 0.9308 - val_f1_micro: 0.9414\n",
      "Epoch 28/30\n",
      "19872/19872 [==============================] - 6s 284us/sample - loss: 0.0498 - f1_macro: 0.9814 - f1_micro: 0.9841 - val_loss: 0.2730 - val_f1_macro: 0.9325 - val_f1_micro: 0.9423\n",
      "Epoch 29/30\n",
      "19872/19872 [==============================] - 6s 280us/sample - loss: 0.0539 - f1_macro: 0.9798 - f1_micro: 0.9828 - val_loss: 0.2712 - val_f1_macro: 0.9248 - val_f1_micro: 0.9346\n",
      "Epoch 30/30\n",
      "19872/19872 [==============================] - 6s 285us/sample - loss: 0.0552 - f1_macro: 0.9795 - f1_micro: 0.9823 - val_loss: 0.2629 - val_f1_macro: 0.9265 - val_f1_micro: 0.9363\n"
     ]
    }
   ],
   "source": [
    "nfolds = params['nfold']\n",
    "loss_CV_folds = [None for i in range(nfolds)]\n",
    "f1_macro_CV_folds = [None for i in range(nfolds)]\n",
    "f1_micro_CV_folds = [None for i in range(nfolds)]\n",
    "\n",
    "for i in tqdm(range(nfolds)):\n",
    "  print(f'Performing KFold iteration on chunk {i+1}/{nfolds}')\n",
    "  CV_block = i\n",
    "  train_blocks = list(range(nfolds))\n",
    "  train_blocks.pop(i)\n",
    "\n",
    "  train = pd.concat([train_folds[j].copy() for j in train_blocks])\n",
    "  CV = train_folds[CV_block].copy()\n",
    "\n",
    "  #extract labels & convert to categorical\n",
    "  labels_train = pd.get_dummies(train['label'])\n",
    "  labels_CV = pd.get_dummies(CV['label'])\n",
    "\n",
    "  #remove labels from train\n",
    "  train.drop(columns=['label'], inplace=True)\n",
    "  CV.drop(columns=['label'], inplace=True)\n",
    "\n",
    "  #apply normalization\n",
    "  train, CV = NormalizeData(PowerData(train), PowerData(CV),PCA_comp=params['PCA_comp'])\n",
    "\n",
    "  print('train shape', train.shape, 'train label shape', labels_train.shape)\n",
    "  print('CV shape', CV.shape, 'CV shape', labels_CV.shape)\n",
    "  STEPS_PER_EPOCH = len(train) // params['batchsize']\n",
    "  print('Epochs :', params['epoch'],'Batch size :', params['batchsize'], 'Step per epochs :', STEPS_PER_EPOCH)\n",
    "\n",
    "\n",
    "  #build model\n",
    "  model = BuildModel(train.shape[1], activation=params['activation'])\n",
    "  adam = tf.keras.optimizers.Adam(lr=params['lr'], beta_1=params['beta1'], beta_2=params['beta2'], amsgrad=params['amsgrad'])\n",
    "  model.compile('adam',loss='categorical_crossentropy', metrics=[f1_macro, f1_micro])\n",
    "\n",
    "  #run model\n",
    "  hist = model.fit(train, labels_train.values, validation_data=(CV, labels_CV.values), \n",
    "          epochs = params['epoch'], verbose = 1, batch_size=params['batchsize'])\n",
    "  \n",
    "  result = model.evaluate(CV, labels_CV.values, batch_size=params['batchsize'])\n",
    "  loss_CV_folds[i] = result[0]\n",
    "  f1_macro_CV_folds[i] = result[1]\n",
    "  f1_micro_CV_folds[i] = result[2]\n",
    "\n",
    "print('CV loss', loss_CV_folds, 'mean', np.asarray(loss_CV_folds).mean())\n",
    "print('CV FScore macro', f1_macro_CV_folds, 'mean', np.asarray(f1_macro_CV_folds).mean())\n",
    "print('CV FScore micro', f1_micro_CV_folds, 'mean', np.asarray(f1_micro_CV_folds).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMeMKKa4tthM"
   },
   "source": [
    "## Perform Training on full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnH9GrRip40S"
   },
   "outputs": [],
   "source": [
    "train = pd.concat([train_folds[i].copy() for i in range(params['nfold'])])\n",
    "\n",
    "#extract labels & convert to categorical\n",
    "labels_train = pd.get_dummies(train['label'])\n",
    "\n",
    "#remove labels from train\n",
    "train.drop(columns=['label'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6jqynxjD2tC0",
    "outputId": "c3d3d715-65f5-4b35-8a81-be84d1a75016"
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "o-u8P-JswUit",
    "outputId": "b7b06abf-7ad8-49af-c14a-9da0ff344af9"
   },
   "outputs": [],
   "source": [
    "#apply normalization\n",
    "train, test = NormalizeData(PowerData(train), PowerData(test),PCA_comp=params['PCA_comp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3564-k_MNMYk",
    "outputId": "10aa48d1-75f1-4455-8e5c-3e7007e54614"
   },
   "outputs": [],
   "source": [
    "print('train shape', train.shape, 'train label shape', labels_train.shape)\n",
    "STEPS_PER_EPOCH = len(train) // params['batchsize']\n",
    "print('Epochs :', params['epoch'],'Batch size :', params['batchsize'], 'Step per epochs :', STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "colab_type": "code",
    "id": "JW0-1-uq3WSX",
    "outputId": "648e764b-120d-43cf-88b5-b8daca5707b2"
   },
   "outputs": [],
   "source": [
    "model = BuildModel(train.shape[1], activation=params['activation'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTkhiSeVZ2v5"
   },
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate=params['lr'], beta_1=params['beta1'], beta_2=params['beta2'], amsgrad=params['amsgrad'])\n",
    "model.compile(adam,loss='categorical_crossentropy', metrics=[f1_macro, f1_micro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JKr8ApNZaDlU",
    "outputId": "274cba91-5c5f-4062-c309-28aed503248d"
   },
   "outputs": [],
   "source": [
    "hist = model.fit(train, labels_train.values, epochs = params['epoch'], batch_size=params['batchsize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "5nmDzhAwotJr",
    "outputId": "69ab0358-cdd8-436d-9e3b-9878df840eed"
   },
   "outputs": [],
   "source": [
    "model.evaluate(train, labels_train.values, verbose=1, batch_size=params['batchsize'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nD7sdJCW2pt5"
   },
   "source": [
    "## Build Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sJytJVS1nD3F",
    "outputId": "9d54abd5-8440-452d-fea8-e0c3bfa23762"
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "54G4mP3hWp_N"
   },
   "outputs": [],
   "source": [
    "#create submission file\n",
    "preds = model.predict(test)\n",
    "pred_df = pd.DataFrame(preds.round().argmax(axis=1), columns=['label'])\n",
    "pred_df.to_csv('submissionNN.csv', index=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O1K89Zax2sLO"
   },
   "source": [
    "## Get model logits for ensembling\n",
    "Finally a bit complex to use for ensembling with other model as the range of logits (before softmax) are differents for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "vglewNn9m9-U",
    "outputId": "c4ee8eb0-0ed4-4c75-b185-064c837c8e94"
   },
   "outputs": [],
   "source": [
    "modelTest = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "testpred = modelTest.predict(test)\n",
    "print(testpred[0])\n",
    "print(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "R7K9WM9lSU1C",
    "outputId": "959b6de9-b742-48a7-e8a0-df5155f620a3"
   },
   "outputs": [],
   "source": [
    "testpred / np.sum(testpred, -1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bDfw25KNWVt9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MailClassNNColab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
